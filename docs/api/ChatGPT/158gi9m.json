{
 "submission_id": "158gi9m",
 "title": "AI reconstructs music from human brain activity it's called \"Brain2Music\" it created by researches at Google",
 "selftext": "A new study called [Brain2Music](https://huggingface.co/papers/2307.11078) demonstrates the reconstruction of music from human brain patterns This work provides a unique window into how the brain interprets and represents music.\n\n**Introducing Brain2Music**\n\n**The core methodology involves:**\n\n* Reconstructing the original clip by either:\n   * Retrieving similar music based on the predicted embedding.\n   * Generating new music conditioned on the embedding using MusicLM, a cutting-edge transformer-based generative model.\n* Predicting a high-level music embedding from fMRI data capturing a subject listening to a music stimulus using linear regression.\n\n**Key Technical Findings**\n\n* **Semantic reconstruction success:**\n   * The reconstructed music semantically resembles the original clips in terms of genre, instrumentation, mood based on human evaluation and quantitative metrics.\n* **Model-brain representation alignment:**\n   * Different components of MusicLM correlate with distinct brain regions, suggesting the AI representations partially mirror those in the human auditory system.\n* **Text embedding-auditory cortex links:**\n   * Purely text-derived embeddings correlate strongly with auditory cortex activity, indicating abstract information is represented in those regions.\n\n**Limitations and Future Work**\n\n**Limitations of the current approach include:**\n\n* Coarse temporal fMRI resolution limits reconstruction quality.\n* Choice of embedding and generation model constrain results.\n\n**Future work could involve:**\n\n* Reconstructing imagined or recalled music.\n* Comparing reconstructions across different subject groups like musicians.\n\n**Implications**\n\n**This AI-powered reconstruction approach enables new insights into:**\n\n* How different facets of music like genre and instrumentation are represented in the brain.\n* Similarities between AI model representations and biological auditory processing.\n* The depth of information contained in non-invasive brain scans.\n\n**TL;DR**\n\nResearchers introduced Brain2Music to reconstruct music from brain scans using AI. MusicLM generates music conditioned on an embedding predicted from fMRI data. Reconstructions semantically resemble original clips but face limitations around embedding choice and fMRI data. The work provides insights into how AI representations align with brain activity.\n\nFull 21 page paper: ([link](https://arxiv.org/pdf/2307.11078.pdf))\n\n**PS:** You can get smarter about AI in 3 minutes by joining one of the [fastest growing AI newsletters.](https://www.theedge.so/subscribe) Join our family of **1000s of professionals from Open AI, Google, Meta, and more.**",
 "created_utc": 1690218376,
 "permalink": "/r/ChatGPT/comments/158gi9m/ai_reconstructs_music_from_human_brain_activity/",
 "score": 57,
 "url": "https://www.reddit.com/r/ChatGPT/comments/158gi9m/ai_reconstructs_music_from_human_brain_activity/",
 "comments": []
}