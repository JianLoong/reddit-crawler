{
 "submission_id": "158jnbs",
 "title": "GPT Weekly - 24th July Edition - Llama 2, Customer care - the first casualty in AI, how to use Llama 2 locally and more",
 "selftext": "This is a recap covering the major news from last week.\n\n* üî•Top 3 news: Llama 2 - the free commercial model, GPT-4 Performance, OpenAI releases\n* üóûÔ∏èInteresting reads - AI regulation in different countries, Fall of Customer Service, Coming war for on-device LLMs and more\n* üßë‚ÄçüéìLearning - Llama 2 resources: Easiest way to use it on Windows, Mac and Ubuntu, Train Llama 2 on local machine and deploying it on M1/M2 Mac\n\nNote: Removed links due to automod. Please visit the page to see the links:\n\n[https://gptweekly.beehiiv.com/p/llama-2-release-gpt4-performance-chatgpt-custom-instructions](https://gptweekly.beehiiv.com/p/llama-2-release-gpt4-performance-chatgpt-custom-instructions)\n\n# üî•Top 3 AI news in the past week\n\n## 1. Commercial and open-source Llama Model\n\nAfter weeks of waiting, Llama-2 finally dropped.\n\n**Salient Features:**\n\n* Llama 2 was trained on **40% more data than LLaMA 1** and has **double the context length**\n* **Three model sizes available - 7B, 13B, 70B**. Pretrained on 2 trillion tokens and **4096 context length**.\n* **Outperforms other open source LLMs on various benchmarks** like HumanEval, one of the popular benchmarks.\n* Partnership with Microsoft. *It seems* ***Microsoft has a finger in every LLM pie.***\n\nThis announcement has its share of controversies.\n\n**First, despite what Meta says the model isn‚Äôt open-source.** There are restrictions on certain users and usage.\n\nAnother perspective is that Meta‚Äôs constant use of ‚Äúopen source‚Äù might be confusing but it‚Äôs okay. Laymen wouldn‚Äôt get the difference between open weights and open source.\n\n**Second, the restriction on using Llama 2‚Äôs output.** Meta doesn‚Äôt want anyone to use Llama 2‚Äôs output to train and improve other LLMs. This is hypocritical and impossible to track.\n\nThe hypocrisy comes from the fact that they have been using other‚Äôs data to train their LLM but don‚Äôt want others to do the same.\n\nThe tracking challenge is how do they ensure no one is using synthetic data from Llama 2? Unless there is a whistleblower, this is going to be impossible.\n\nThis is important because even Microsoft and OpenAI realize **there is a limit to human data to train LLM**.\n\n**The main takeaway** Open-source and commercial-free models are the future. Llama 2 is a step in the right direction. Hopefully, OpenAI is going to release their own version soon.\n\n## 2. GPT-4 Performance\n\nDiscussion on **GPT-4‚Äôs performance** has been on everyone‚Äôs mind. A lot of people keep saying it is dumber but either **don‚Äôt have proof** or their proof doesn‚Äôt work because of the **non-deterministic nature of GPT-4 response**. There is always a chance that one response is dumber than the other.\n\nLast week a study tried to quantify and measure GPT-4‚Äôs performance over the past 4 months. While the title of the study is ‚ÄúHow is ChatGPT's behavior changing over time‚Äù many took this as proof that GPT-4 has deteriorated.\n\nTo measure GPT-4 performance authors **used snapshots**. **OpenAI maintains two snapshots of GPT-4 - a March version and a June version**. The authors used a set of standard questions to measure the performance variability.\n\nThe authors used 500 math problems and chain-of-thought prompts on both the versions. The March version got 488 questions right (97.6%) while the June verison got only 12 questions right. **That is 97.6% right in March vs 2.7% right in June.**\n\nThey also used 50 coding questions from LeetCode to measure the programming performance. And measured how many GPT-4 answers ran without any changes. **For March version 52% of the generated code worked and for June version 10% of the code worked.**\n\nLot of people took this as proof that GPT-4 performance has gone down. But there is **no claim** **of performance degradation.**\n\nThis study shows there is **inherent bias** in the March vs June model. For the math problem where the worst performance was seen, one keeps saying every number is prime while the other model says every number is composite. So, the **performance might not be bad overall just that there is training bias.**\n\n## 3. OpenAI Releases and Announcements\n\nLast week we saw important announcements from OpenAI:\n\n**First, the Android app is one the way.** You need to pre-register to download it as soon as it is available.\n\n**Second, custom instructions for ChatGPT**\\*\\*.\\*\\* Have you ever wanted ChatGPT to respond in a particular way? Like every response needs to have: A pros and cons list. Or a bullet points list? Or respond in a particular tone and tenor? Now instead of telling ChatGPT - ‚ÄúNow respond in X voice or respond with bullet points‚Äù during the chat, you make it a default setting.\n\nYou can enable this by going to Settings: (Click on your user name)\n\nOnce done you should see a Custom instructions option (when clicking user name).\n\n**Third, increased messages for GPT-4.** Now you can send 50 messages every 3 hours. This is a 2x increase over the previous 25 message limit. Though you wonder if people believe performance is going down then what is the point?\n\n# üóûÔ∏è10 AI news highlights and interesting reads\n\n1. White House reached an agreement with tech giants on managing the risks from AI. It is a voluntary **commitment.** It underscores how different regions and countries are approaching regulation. **The US is more about self-regulation, the EU wants consumer protection and safety, China wants state control**\\*\\*.\\*\\*\n2. The first domino to fall in the AI race seems to be customer service. Just replacing the customer service teams with chatbots. First, it was an Indian startup Dukaan which **replaced 90% of the team with chatbots**. Now, Shopify is doing the same **but with NDAs to avoid bad press.** And the same goes for call center workers who are battling with AI.\n3. **Apple is testing Apple-GPT.** They also have built an **internal framework called Ajax**. New technology always leads to people writing frameworks and re-inventing the wheel. So, this is going to be interesting.\n4. In the meantime, **Meta is also working with Qualcomm to enable on-device use of Llama 2.** Currently, models are run in the cloud and have privacy and security concerns. **An on-device AI is secure, more private and provides more scope for personalization.**\n5. An entirely AI made South Park Episode was created. They used **GPT-4 to generate the dialogue and text, diffusion model to generate the character and voice cloning to provide voice.** This is an achievement in combining multiple AI techniques to create a unified flow and product. **It is both exciting and dangerous.**\n6. Who‚Äôs next on the AI chopping block? The news writers, maybe? **Google showcased an AI tool which can write news articles.** **Internally it is called Genesis (not so subtle nod to Terminator: Genisys?)** News companies have been under lots of pressure and this doesn‚Äôt help. It has left people unsettled. While many chose to not comment, this is concerning. Especially, when you have executives **pushing for more AI content**.\n7. Open Source is digesting the AI research results quickly. Now researchers need to learn how to balance performance vs practicality of solutions.\n8. LLMs might pose a threat to digital conversations. Researchers found **Stackoverflow contributions are down 16%.** Though you have to remember that SO has been a difficult place for newbies. With ChatGPT providing an easier answer, why would they want to go to SO?\n9. **34000% growth in AI projects** says Replit. Tell me there is no hype.\n10. LangSmith, a **unified platform for debugging, testing, evaluating, and monitoring your LLM applications**\n\n# üßë‚Äçüéì3 Learning Resources\n\n1. The easiest way I found to run Llama 2 locally is to utilize GPT4All. Here are the short steps:\n   1. Download the [GPT4All installer](https://gpt4all.io/index.html)\n   2. Download the GGML version of the Llama Model. For example the [7B Model](https://huggingface.co/TheBloke/Llama-2-7B-GGML) ([Other GGML versions](https://huggingface.co/TheBloke))\n   3. For local use it is better to download a [lower quantized model](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q4_0.bin). This should save some RAM and make the experience smoother.\n   4. Go to the installation directory. Place the downloaded file into the ‚Äúmodels‚Äù folder.\n   5. Start GPT4All and at the top you should see an option to select the model.\n   6. Keep in mind the instructions for Llama 2 are odd. It is not a simple prompt format like ChatGPT. Check the [prompt template](https://gpus.llm-utils.org/llama-2-prompt-template/).\n2. [Run Llama 2 on M1/M2 Mac with GPU](https://gist.github.com/adrienbrault/b76631c56c736def9bc1bc2167b5d129)\n3. [Finetune Llama 2 on a local machine.](https://www.youtube.com/watch?v=3fsn19OI_C8)\n\nThat‚Äôs it folks. Thank you for reading and have a great week ahead.\n\n**If you are interested in a focused weekly recap delivered to your inbox on Mondays you can**[ subscribe here. It is FREE!](https://gptweekly.beehiiv.com/subscribe)",
 "created_utc": 1690225336,
 "permalink": "/r/ChatGPT/comments/158jnbs/gpt_weekly_24th_july_edition_llama_2_customer/",
 "score": 81,
 "url": "https://www.reddit.com/r/ChatGPT/comments/158jnbs/gpt_weekly_24th_july_edition_llama_2_customer/",
 "comments": [
  {
   "comment_id": "jta9qt0",
   "message": "***Hey /u/level6-killjoy, if your post is a ChatGPT conversation screenshot, please reply with the [conversation link](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq) or prompt. Thanks!***\n\n***We have a [public discord server](https://discord.gg/r-chatgpt-1050422060352024636). There's a free Chatgpt bot, Open Assistant bot (Open-source model), AI image generator bot, Perplexity AI bot, \u0026amp;#x1F916; GPT-4 bot ([Now with Visual capabilities (cloud vision)!](https://cdn.discordapp.com/attachments/812770754025488386/1095397431404920902/image0.jpg)) and channel for latest prompts! New Addition: Adobe Firefly bot and Eleven Labs cloning bot! [So why not join us?](https://discord.com/servers/1050422060352024636)***\n\n***NEW: [Text-to-presentation contest | $6500 prize pool](https://redd.it/14si211/)***\n\nPSA: For any Chatgpt-related issues email support@openai.com\n\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/ChatGPT) if you have any questions or concerns.*",
   "created_utc": 1690225337,
   "score": 1,
   "submission_id": "158jnbs"
  },
  {
   "comment_id": "jta9s2j",
   "message": "#tl;dr\n\nLast week's major AI news included the release of Llama 2, a commercial and open-source model that outperforms other models on various benchmarks but comes with restrictions on usage. There was also discussion on the performance of GPT-4, with a study suggesting a drop in performance but no claims of degradation. OpenAI made announcements about an upcoming Android app, custom instructions for ChatGPT, and an increase in the number of messages for GPT-4. Other interesting reads covered topics such as AI regulation, the use of chatbots in customer service, and the development of on-device AI.\n\n*I am a smart robot and this summary was automatic. This tl;dr is 93.37% shorter than the post I'm replying to.*",
   "created_utc": 1690225350,
   "score": 1,
   "submission_id": "158jnbs"
  },
  {
   "comment_id": "jtaya70",
   "message": "Awwwesome post. Keep it up! Please and thank you",
   "created_utc": 1690234810,
   "score": 1,
   "submission_id": "158jnbs"
  }
 ]
}